[id="about-node-selector_{context}"]

= About node selector

There are a variety of reasons why you might want to restrict the nodes where
{rhos_prev_long} ({OpenStackShort}) services can be placed:

* Hardware requirements: System memory, Disk space, Cores, HBAs
* Limit the impact of the {OpenStackShort} services on other {OpenShift} workloads.
* Avoid collocating {OpenStackShort} services.

The mechanism provided by the {OpenStackShort} operators to achieve this is through the
use of labels.

You either label the {OpenShiftShort} nodes or use existing labels, and then use those labels in the {OpenStackShort} manifests in the
`nodeSelector` field.

The `nodeSelector` field in the {OpenStackShort} manifests follows the standard
{OpenShiftShort} `nodeSelector` field. For more information, see link:https://docs.openshift.com/container-platform/4.15/nodes/scheduling/nodes-scheduler-node-selectors.html[About node selectors] in _OpenShift Container Platform 4.15 Documentation_.

This field is present at all the different levels of the {OpenStackShort} manifests:

* Deployment: The `OpenStackControlPlane` object.
* Component: For example the `cinder` element in the `OpenStackControlPlane`.
* Service: For example the `cinderVolume` element within the `cinder` element
in the `OpenStackControlPlane`.

This allows a fine grained control of the placement of the {OpenStackShort} services
with minimal repetition.

Values of the `nodeSelector` are propagated to the next levels unless they are
overwritten. This means that a `nodeSelector` value at the deployment level will
affect all the {OpenStackShort} services.

For example, you can add label `type: openstack` to any 3 {OpenShiftShort} nodes:

----
oc label nodes worker0 type=openstack
oc label nodes worker1 type=openstack
oc label nodes worker2 type=openstack
----

And then in our `OpenStackControlPlane` you can use the label to place all the
services in those 3 nodes:

[source,bash,role=execute,subs=attributes]
----
apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: ocs-external-storagecluster-ceph-rbd
  nodeSelector:
    type: openstack
< . . . >
----

You can use the selector for specific services. For example, you might want to place your the Block Storage service (cinder) volume and backup services on certain nodes if you are using FC and only have HBAs on a subset of
nodes. The following example assumes that you have the label `fc_card: true`:

[source,bash,role=execute,subs=attributes]
----
apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: ocs-external-storagecluster-ceph-rbd
  cinder:
    template:
      cinderVolumes:
          pure_fc:
            nodeSelector:
              fc_card: true
< . . . >
          lvm-iscsi:
            nodeSelector:
              fc_card: true
< . . . >
      cinderBackup:
          nodeSelector:
            fc_card: true
< . . . >
----

The Block Storage service operator does not currently have the possibility of defining
the `nodeSelector` in `cinderVolumes`, so you need to specify it on each of the
backends.

It is possible to leverage labels added by the  Node Feature Discovery (NFD) Operator to place {OpenStackShort} services. For more information, see link:https://docs.openshift.com/container-platform/4.13/hardware_enablement/psap-node-feature-discovery-operator.html[Node Feature Discovery Operator] in _OpenShift Container Platform 4.15 Documentation_.
